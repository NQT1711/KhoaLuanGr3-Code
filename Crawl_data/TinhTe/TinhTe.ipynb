{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries & Set Options for Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import random\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "user_agent = ua.random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mở trình duyệt Google Chrome\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito') # Tab ẩn danh\n",
    "options.add_argument(\"--start-maximized\") # Full window\n",
    "options.add_argument('headless') # Không hiển thị chrome\n",
    "options.add_argument(f'user-agent={user_agent}')\n",
    "s = Service('../chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_agents = [\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/96.0.1054.62\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/95.0\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_1) AppleWebKit/537.36 (KHTML, like Gecko) Edge/96.0.1054.62\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_5_1) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/95.0\",\n",
    "#     \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Mobile/15E148 Safari/604.1\",\n",
    "#     \"Mozilla/5.0 (Linux; Android 11; SM-G988U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Mobile Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Linux; Android 11; Pixel 4 XL) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Mobile Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/95.0\",\n",
    "#     \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; Trident/7.0; rv:11.0) like Gecko\",\n",
    "#     \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko\",\n",
    "#     \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62\",\n",
    "#     \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:95.0) Gecko/20100101 Firefox/95.0\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 OPR/83.0.4787.63\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# def get_random_proxy():\n",
    "#     proxies = [\n",
    "#         {\"http\": \"http://123.30.154.171:7777\", \"https\": \"http://123.30.154.171:7777\"},\n",
    "#         {\"http\": \"http://165.154.186.232:80\", \"https\": \"http://165.154.186.232:80\"},\n",
    "#         {\"http\": \"http://113.161.131.43:80\", \"https\": \"http://113.161.131.43:80\"},\n",
    "#         {\"http\": \"http://117.4.50.142:32650\", \"https\": \"http://117.4.50.142:32650\"},\n",
    "#     ]\n",
    "#     return random.choice(proxies)\n",
    "# # options.add_argument(f'--proxy-server={get_random_proxy()[\"http\"]}')\n",
    "# options.add_argument(f'user-agent={random.choice(user_agents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Url PosT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_post(keyword):\n",
    "    url_page = 'https://tinhte.vn/'\n",
    "    driver = webdriver.Chrome(service=s, options=options)\n",
    "    driver.get(url_page)\n",
    "    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"gsc-i-id1\"]')))\n",
    "    time.sleep(random.randint(3,7))\n",
    "\n",
    "    # Send searchword into Input Box/ Inputing search word into box\n",
    "    box = driver.find_element(By.XPATH,'//*[@id=\"gsc-i-id1\"]')\n",
    "    box.send_keys(keyword)\n",
    "    time.sleep(random.randint(3,9))\n",
    "    box.send_keys(Keys.ENTER)\n",
    "    print(f'Crawling posts related to {keyword}')\n",
    "    \n",
    "    # Get urls of all pages\n",
    "    df_link=[]\n",
    "    for i in range(1,11):\n",
    "        print(f'Crawling Page {i} of {keyword}')\n",
    "        time.sleep(random.randint(5, 7))\n",
    "        driver.find_element(By.XPATH,'//*[@id=\"___gcse_0\"]/div/div/div[1]/div[6]')\n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "        soup2 = soup.select('div[class=\"gs-title\"]')\n",
    "\n",
    "        # Get all urls in a page (10)\n",
    "        for item in range(len(soup2)-1):\n",
    "            i2 = soup2[item].find('a').get('href')\n",
    "            df_link.append(i2)\n",
    "\n",
    "        # Don't click into page 1.\n",
    "        if i <10:\n",
    "            xpath = f'//*[@id=\"___gcse_0\"]/div/div/div[1]/div[6]/div[2]/div/div/div[2]/div/div[{i+1}]'\n",
    "            WebDriverWait(driver, 11) \n",
    "            driver.find_element(By.XPATH,xpath).click()\n",
    "\n",
    "    driver.close()\n",
    "    return df_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword=['Iphone 15 pro max','Samsung S23 Ultra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crawl Iphone 15 Pro Max Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling posts related to Iphone 15 pro max\n",
      "Crawling Page 1 of Iphone 15 pro max\n",
      "Crawling Page 2 of Iphone 15 pro max\n",
      "Crawling Page 3 of Iphone 15 pro max\n",
      "Crawling Page 4 of Iphone 15 pro max\n",
      "Crawling Page 5 of Iphone 15 pro max\n",
      "Crawling Page 6 of Iphone 15 pro max\n",
      "Crawling Page 7 of Iphone 15 pro max\n",
      "Crawling Page 8 of Iphone 15 pro max\n",
      "Crawling Page 9 of Iphone 15 pro max\n",
      "Crawling Page 10 of Iphone 15 pro max\n"
     ]
    }
   ],
   "source": [
    "df_link1 = get_url_post(keyword[0])\n",
    "df=pd.DataFrame({'Link':df_link1})\n",
    "save_path = \"../../Data/TinhTe/TinhTe_Iphone15pm_Link.csv\"\n",
    "df.to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crawl Samsung S23 Ultra Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling posts related to Samsung S23 Ultra\n",
      "Crawling Page 1 of Samsung S23 Ultra\n",
      "Crawling Page 2 of Samsung S23 Ultra\n",
      "Crawling Page 3 of Samsung S23 Ultra\n",
      "Crawling Page 4 of Samsung S23 Ultra\n",
      "Crawling Page 5 of Samsung S23 Ultra\n",
      "Crawling Page 6 of Samsung S23 Ultra\n",
      "Crawling Page 7 of Samsung S23 Ultra\n",
      "Crawling Page 8 of Samsung S23 Ultra\n",
      "Crawling Page 9 of Samsung S23 Ultra\n",
      "Crawling Page 10 of Samsung S23 Ultra\n"
     ]
    }
   ],
   "source": [
    "df_link2 = get_url_post(keyword[1])\n",
    "df=pd.DataFrame({'Link':df_link2})\n",
    "save_path = '../../Data/TinhTe/TinhTe_SamsungS23u_Link.csv'\n",
    "df.to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl Post Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_post(soup):\n",
    "    soup_ct = soup.select_one('article')\n",
    "    soup_ct1 = soup_ct.select('span[class=\"xf-body-paragraph\"]')\n",
    "    content=[]\n",
    "    for child in soup_ct1:\n",
    "        info_div = child.find(\"div\",class_=\"info\")\n",
    "        if info_div:\n",
    "            info_div.extract()\n",
    "        content.append(child.getText(separator='\\n' ,strip=True))\n",
    "    return content\n",
    "\n",
    "def get_info_post(url):\n",
    "    respone = requests.get(url)\n",
    "    list_info =[]\n",
    "    if respone.status_code ==200:\n",
    "        soup = BeautifulSoup(respone.text,'lxml')\n",
    "        tt = soup.select_one('div[class=\"jsx-89440 thread-title\"] > h1')\n",
    "        if tt != None:\n",
    "            title=tt.text\n",
    "            author = soup.select_one('div[class=\"jsx-89440 author-name\"]>a').text\n",
    "            post_date = soup.select_one('div[class=\"jsx-89440 date-comment-view\"]>span[class=\"jsx-89440 date\"]').text\n",
    "            comments_count = soup.select_one('div[class=\"jsx-89440 date-comment-view\"]>span[class=\"jsx-89440 comment\"]>span').text\n",
    "            if 'page' not in url:\n",
    "                content_list =  get_content_post(soup)\n",
    "                content = '\\n'.join(content_list)\n",
    "            else:\n",
    "                content = ''\n",
    "            list_info.append([url,title,author,post_date,comments_count,content])\n",
    "\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    return list_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_post(start,end,df_save,df_source,columns):\n",
    "    for url in df_source['Link'][start-1:end]:\n",
    "        data = get_info_post(url)\n",
    "        df_save = pd.concat([df_save, pd.DataFrame(data,columns=columns)], ignore_index=True)\n",
    "        time.sleep(random.randint(3,5))\n",
    "    return df_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Post Information for Iphone 15 Promax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_ip15pm = pd.read_csv('../../Data/TinhTe/TinhTe_Iphone15pm_Link.csv')\n",
    "columns = ['Link','Title', 'Author', 'Post Date', 'Comments Count', 'Content']\n",
    "df_info_ip15pm = pd.DataFrame(columns=columns)\n",
    "start =1\n",
    "end=100\n",
    "df = number_post(start,end,df_info_ip15pm,df_link_ip15pm,columns)\n",
    "save_path = \"../../Data/TinhTe/TinhTe_Iphone15pm_InfoPosts2.csv\"\n",
    "df.to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Post Information for Samsung S23 Ultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_sss23u = pd.read_csv('../../Data/TinhTe/TinhTe_SamsungS23u_Link.csv')\n",
    "columns = ['Link','Title', 'Author', 'Post Date', 'Comments Count', 'Content']\n",
    "df_info_sss23u = pd.DataFrame(columns=columns)\n",
    "start =1\n",
    "end=100\n",
    "df = number_post(start,end,df_info_sss23u,df_link_sss23u,columns)\n",
    "save_path = \"../../Data/TinhTe/TinhTe_SamsungS23u_InfoPosts.csv\"\n",
    "\n",
    "df.to_csv(save_path,index=False)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Comments for Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_older_replies(driver):\n",
    "    while True:\n",
    "        soup_handle_rl = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        click_elms = soup_handle_rl.select('button[class=\"jsx-691990575 thread-comments__load-more\"]')\n",
    "        if len(click_elms)==0:\n",
    "            break\n",
    "        else:\n",
    "            for i in range(35):\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH,f'//*[@id=\"__next\"]/div[1]/div/div[2]/div[2]/div[1]/div/div/div[1]/div[3]/div[2]/div/div[{i}]/div/div[2]/div[3]/button').click()\n",
    "                    time.sleep(random.randint(2,4))\n",
    "                except:              \n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_comments(url,driver):\n",
    "    list_cmt = [] # create a list to save all comments\n",
    "    # time.sleep(random.randint(3,7))\n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    main_cmt = soup.select('div[class=\"jsx-691990575 thread-comment__wrapper\"]')\n",
    "    for item in main_cmt:\n",
    "        first_child = item.find_all()[0]\n",
    "        date = first_child.find('a',class_='jsx-691990575 thread-comment__date').text\n",
    "\n",
    "        second_child = item.find_all('div',class_='jsx-4267282249 xfBodyContainer')\n",
    "        for i in second_child:\n",
    "            for tag in i.find_all(['div', 'a'], {'class': ['attribution', 'bbCodeQuote', 'NoOverlay']}):\n",
    "                tag.decompose()\n",
    "            cmt = i.find('div', class_='jsx-4267282249 xfBody').text.strip()\n",
    "            list_cmt.append([url,date,cmt])\n",
    "\n",
    "    return list_cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_func(df_source,start,end):\n",
    "\n",
    "    summary = pd.DataFrame({'Link':[],'Date':[],'Comment':[]}) # create a df to save all comments\n",
    "    driver = webdriver.Chrome(service=s, options=options)\n",
    "    for url in df_source['Link'][start-1:end]:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver,60).until(EC.presence_of_element_located(\n",
    "                (By.XPATH,'//*[@id=\"__next\"]/div[1]/div/div[2]/div[2]/div[1]/div/div/div[1]/div[3]/div[1]/a/div')))\n",
    "        \n",
    "        soup_temp = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            num_page = soup_temp.select('div[class=\"jsx-2305813501 pagination--pages\"]>a[class=\"jsx-2305813501 page\"]')[0].text                \n",
    "        except:\n",
    "            num_page =0\n",
    "        \n",
    "        # click_elms = soup_temp.select('button[class=\"jsx-691990575 thread-comments__load-more\"]')\n",
    "\n",
    "        if num_page!=0:\n",
    "            for i_page in range(1,int(num_page)+1):\n",
    "                if i_page !=1:\n",
    "                    new_url = f'{url}/page-{i_page}'\n",
    "                else:\n",
    "                    new_url = url\n",
    "                print(f'Crawling page: {new_url}')\n",
    "                driver.get(new_url)\n",
    "                WebDriverWait(driver,60).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"__next\"]/div[1]/div/div[2]/div[2]/div[1]/div/div/div[1]/div[3]/div[1]/a/div')))\n",
    "\n",
    "                click_elms = soup_temp.select('button[class=\"jsx-691990575 thread-comments__load-more\"]')\n",
    "                if len(click_elms) !=0:\n",
    "                    handle_older_replies(driver)\n",
    "                \n",
    "                list_cmt = get_comments(url,driver)\n",
    "                df_sub_cmt = pd.DataFrame(list_cmt,columns=['Link','Date','Comment'])\n",
    "                summary = pd.concat([summary,df_sub_cmt])\n",
    "                # summary.append([url,date2,cmt2])\n",
    "                time.sleep(random.randint(2,3))\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get comments for posts on the Iphone 15 Pro Max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_ip15pm = pd.read_csv('./TinhTe_Iphone15pm_Link.csv')\n",
    "df_ip15 = main_func(df_link_ip15pm,1,100)\n",
    "save_path = \"../../Data/TinhTe/TinhTe_Iphone15pm_Comments.csv\"\n",
    "df_ip15.to_csv(save_path,index=False)\n",
    "winsound.Beep(freq,duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get comments for posts on the Samsung S23 Ultra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326/\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-2\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-3\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-4\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-5\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-6\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-7\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-8\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-9\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-10\n",
      "Crawling page: https://tinhte.vn/thread/tren-tay-samsung-galaxy-s23-ultra-minh-da-bi-lua.3629326//page-11\n",
      "Crawling page: https://tinhte.vn/thread/moi-nguoi-danh-gia-sao-ve-samsung-s23-ultra-8gb-ram.3640898/\n",
      "Crawling page: https://tinhte.vn/thread/moi-nguoi-danh-gia-sao-ve-samsung-s23-ultra-8gb-ram.3640898//page-2\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395/\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-2\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-3\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-4\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-5\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-6\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-7\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-8\n",
      "Crawling page: https://tinhte.vn/thread/galaxy-s23-ultra-co-bui-trong-camera-va-samsung-tra-loi-do-la-dieu-binh-thuong.3639395//page-9\n"
     ]
    }
   ],
   "source": [
    "df_link_sss23u = pd.read_csv('./TinhTe_SamsungS23u_Link.csv')\n",
    "df_ss23u = main_func(df_link_sss23u,1,100)\n",
    "save_path = \"../../Data/TinhTe/TinhTe_SamsungS23u_Comments.csv\"\n",
    "df_ss23u.to_csv(save_path, index=False)\n",
    "winsound.Beep(freq,duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
