{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import emoji\n",
    "# import pyvi\n",
    "from collections import Counter\n",
    "# from pyvi import ViTokenizer, ViPosTagger\n",
    "from nltk.corpus import words\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_accent_vietnamese(s): # Xóa dấu của từ\n",
    "    s = re.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', s)\n",
    "    s = re.sub(r'[èéẹẻẽêềếệểễ]', 'e', s)\n",
    "    s = re.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', s)\n",
    "    s = re.sub(r'[ìíịỉĩ]', 'i', s)\n",
    "    s = re.sub(r'[ùúụủũưừứựửữ]', 'u', s)\n",
    "    s = re.sub(r'[ỳýỵỷỹ]', 'y', s)\n",
    "    s = re.sub(r'[đ]', 'd', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accent(text):\n",
    "    accent_character = 'àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễòóọỏõôồốộổỗơờớợởỡìíịỉĩùúụủũưừứựửữỳýỵỷỹđ'\n",
    "    kq = False\n",
    "\n",
    "    for t in list(text):\n",
    "        if t in accent_character:\n",
    "            kq = True\n",
    "    \n",
    "    return kq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_replicated(text): # Xóa chữ cái bị lặp\n",
    "    # Bỏ qua từ đặc biệt\n",
    "    special = ['ưu', 'ứu', 'ừu', 'ựu', 'ửu', 'ữu']\n",
    "    drop_special = {}\n",
    "    s_num = 0\n",
    "    for s_word in special:\n",
    "        if s_word in text:\n",
    "            drop_special[s_word] = 'Special_%s'%s_num\n",
    "            text = text.replace(s_word, 'Special_%s'%s_num)\n",
    "            s_num += 1\n",
    "\n",
    "    # Xóa dấu chữ cái\n",
    "    accent_character = 'àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễòóọỏõôồốộổỗơờớợởỡìíịỉĩùúụủũưừứựửữỳýỵỷỹđ'\n",
    "    dict_accent_word = {}\n",
    "    for c in text:\n",
    "        if c in accent_character:\n",
    "            p = [pos for pos, char in enumerate(text) if char == c]\n",
    "            dict_accent_word[c] = p\n",
    "    text = no_accent_vietnamese(text)\n",
    "\n",
    "    # Check các chữ cái lặp\n",
    "    count = Counter(text)\n",
    "    check_rep_character = [k for k, v in count.items() if v > 1]\n",
    "    remove_pos = {}\n",
    "    for character in check_rep_character:\n",
    "        positions = [pos for pos, char in enumerate(text) if char == character]\n",
    "        remove_list = []\n",
    "        for i in range(len(positions) - 1):\n",
    "            if ((positions[i+1] - positions[i]) == 1):\n",
    "                remove_list.append(positions[i])\n",
    "                remove_list.append(positions[i+1])\n",
    "\n",
    "        if remove_list != []:\n",
    "            remove_pos[character] = list(set(remove_list))\n",
    "\n",
    "    # Gán lại dấu chữ cái\n",
    "    text = list(text)\n",
    "    for k, v in dict_accent_word.items():\n",
    "        for i in v:\n",
    "            text[i] = k\n",
    "\n",
    "    # Loại bỏ chữ cái lặp\n",
    "    for c, list_pos in remove_pos.items():\n",
    "        for k, v in dict_accent_word.items():\n",
    "            compare = set(list_pos).intersection(v)\n",
    "            if len(compare) != 0:\n",
    "                for i in compare:\n",
    "                    list_pos.remove(i)\n",
    "                for remove_index in list_pos:\n",
    "                    text[remove_index] = ''\n",
    "            else:\n",
    "                for remove_index in list_pos[1:]:\n",
    "                    text[remove_index] = ''\n",
    "\n",
    "    text = ''.join(text)\n",
    "\n",
    "    # Thay lại từ đặc biệt\n",
    "    for k, v in drop_special.items():\n",
    "        text = text.replace(v, k)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_abbreviations(text):\n",
    "    abbreviations = pd.read_excel('abbreviations.xlsx')\n",
    "    remove_abbreviations_list = []\n",
    "    for t in text.split(' '):\n",
    "        if t in abbreviations['abbreviation'].values:\n",
    "            remove = abbreviations[abbreviations['abbreviation']==t]['word'].values[0]\n",
    "            remove_abbreviations_list.append(remove)\n",
    "        else:\n",
    "            remove_abbreviations_list.append(t)\n",
    "\n",
    "    text_pre = ' '.join(remove_abbreviations_list)\n",
    "    return text_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_name_ngram():\n",
    "    product_df = pd.read_csv('..\\Data\\Preprocessed_data\\Product.csv')\n",
    "    ignore_product_name = ['điện_thoại', ]\n",
    "\n",
    "    for brand in product_df['Brand'].unique():\n",
    "        ignore_product_name.append(brand.lower())\n",
    "\n",
    "    for product in product_df['ProductName'].values:\n",
    "        product = re.sub(\"\\d+\", \" \", product.lower())\n",
    "        product = ' '.join(product.split())\n",
    "        for num in range(1, 3):\n",
    "            n_grams = ngrams(product.split(' '), num)\n",
    "            for n in ['_'.join(grams) for grams in n_grams]:\n",
    "                ignore_product_name.append(n)\n",
    "                ignore_product_name.append(n.replace('_', ' '))\n",
    "\n",
    "    return list(set(ignore_product_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text , ignore_list=[]):\n",
    "    # Biến chữ hoa thành chữ thường\n",
    "    text_pre = text.lower()\n",
    "\n",
    "    # Loại bỏ url\n",
    "    text_pre = re.sub(r'http\\S+', '', text_pre)\n",
    "\n",
    "    # Loại bỏ tên miền\n",
    "    text_pre = re.sub(r\"[^\\s]*\\.(com|vn|net)\", '', text_pre)\n",
    "\n",
    "    # Loại bỏ hashtag\n",
    "    text_pre = ' '.join(re.sub(\"(#[A-Za-z0-9]+)\", \" \", text_pre).split())\n",
    "\n",
    "    # Loại bỏ user mentions @\n",
    "    text_pre = ' '.join(re.sub(\"(@[A-Za-z0-9]+)\", \" \", text_pre).split())\n",
    "\n",
    "    # Loại bỏ dấu xuống dòng \\r, \\n và tab \\t\n",
    "    text_pre = text_pre.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "    # Loại bỏ emoji\n",
    "    text_pre = emoji.demojize(text_pre, delimiters=(\"\", \" \"))\n",
    "    text_pre = ' '.join([t for t in text_pre.split(' ') if '_' not in t])\n",
    "\n",
    "    # Loại bỏ dấu câu và kí tự\n",
    "    punc = punctuation\n",
    "    punc += '“”…►'\n",
    "    for c in punc:\n",
    "        text_pre = text_pre.replace(c,' ')\n",
    "    text_pre = \" \".join(text_pre.split())\n",
    "\n",
    "    # Loại bỏ chữ bị lặp\n",
    "    text_pre = remove_replicated(text_pre)\n",
    "    \n",
    "    # Loại bỏ chữ số\n",
    "    text_pre = re.sub(\"\\d+\", \" \", text_pre)\n",
    "\n",
    "    # Loại bỏ từ viết tắt và viết sai\n",
    "    text_pre = remove_abbreviations(text_pre)\n",
    "\n",
    "    # Xử lý từ phủ định\n",
    "\n",
    "    # Loại bỏ stopword\n",
    "    f = open(r\"vietnamese-stopwords_edit.txt\", \"r\", encoding=\"utf-8\")\n",
    "    List_StopWords = f.read().split(\"\\n\")\n",
    "    if ignore_list != []:\n",
    "        List_StopWords += ignore_list\n",
    "    text_pre=\" \".join(text for text in text_pre.split() if text not in List_StopWords)\n",
    "\n",
    "    # Tokenize\n",
    "    text_pre = ViTokenizer.tokenize(text_pre)\n",
    "\n",
    "    # Pos tagging\n",
    "    pos_tagger = ViPosTagger.postagging(text_pre)\n",
    "    text_pre = []\n",
    "    for i in range(len(pos_tagger[0])):\n",
    "        if pos_tagger[1][i] in ['N'] and len(pos_tagger[0][i]) > 2 and pos_tagger[0][i] not in product_name_ngram():\n",
    "            text_pre.append(pos_tagger[0][i])\n",
    "    text_pre = ' '.join(text_pre)\n",
    "    \n",
    "    return text_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom_reviews = pd.read_csv(r'..\\..\\Data\\Preprocessed_data\\EcomReviews.csv')['Review'].dropna().reset_index(drop=True)\n",
    "sm_reviews = pd.read_csv(r'..\\..\\Data\\Preprocessed_data\\SocialMediaReviews.csv')['Review'].dropna().reset_index(drop=True)\n",
    "reviews = [item for item in list(ecom_reviews)] + [item for item in list(sm_reviews)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
